# deepseekr1maker/rewards/accuracy_reward.py
import re
import math
from typing import List, Dict, Any, Optional, Union
from latex2sympy2_extended import NormalizationConfig
from math_verify import LatexExtractionConfig, parse, verify

def accuracy_reward(completions, return_diagnostics=False, **kwargs):
    """
    Reward function to verify if the model's answer is mathematically equivalent to the correct solution.
    Uses latex2sympy2 for parsing and math_verify for validation.
    
    Args:
        completions: List of completions generated by the model
        return_diagnostics: Whether to return detailed diagnostic information
        **kwargs: Additional arguments, including 'solution'
    
    Returns:
        If return_diagnostics=False:
            List of reward scores (1.0 for correct, 0.0 for incorrect, 0.5 if non-analyzable)
        If return_diagnostics=True:
            List of dictionaries containing reward scores and diagnostic information
    """
    # Extract responses
    contents = [completion[0]["content"] for completion in completions]
    rewards = []
    diagnostics = []

    solutions = kwargs.get("solution")  # Get solutions from kwargs
    verbose = kwargs.get("verbose", False)  # Verbosity flag

    if solutions is None:
        # Neutral reward if no solution provided
        if return_diagnostics:
            return [{"reward": 0.5, "status": "no_reference_solution", "details": "No reference solution provided"} for _ in completions]
        return [0.5] * len(completions)
    
    for idx, (content, sol) in enumerate(zip(contents, solutions)):
        diagnostic_info = {
            "status": "unknown",
            "details": "",
            "parsed_solution": None,
            "parsed_answer": None,
            "verification_method": None
        }
        
        # Parse the reference solution with multiple extraction configs for robustness
        gold_parsed = parse(
            sol, 
            extraction_mode="all",  # Try to extract all possible answers
            extraction_config=[
                LatexExtractionConfig(),  # Default config
                LatexExtractionConfig(  # More permissive config
                    normalization_config=NormalizationConfig(
                        nits=False,
                        malformed_operators=True,
                        basic_latex=True,
                        equations=True,
                        boxed="all",
                        units=True,
                    )
                )
            ]
        )
        
        if not gold_parsed:
            # If reference solution cannot be parsed, assign neutral reward (0.5)
            reward = 0.5
            diagnostic_info["status"] = "reference_parse_failure"
            diagnostic_info["details"] = f"Failed to parse reference solution: {sol}"
            if verbose:
                print(f"Warning: Failed to parse reference solution: {sol}")
        else:
            diagnostic_info["parsed_solution"] = str(gold_parsed)
            
            # Parse the model's answer with multiple extraction configs
            answer_parsed = parse(
                content,
                extraction_config=[
                    LatexExtractionConfig(),  # Try default first
                    LatexExtractionConfig(  # Then try more permissive config
                        normalization_config=NormalizationConfig(
                            nits=False,
                            malformed_operators=True,
                            basic_latex=True,
                            equations=True,
                            boxed="all",
                            units=True,
                        ),
                        boxed_match_priority=0,
                        try_extract_without_anchor=True,  # Try without requiring anchors
                    )
                ],
                extraction_mode="all",  # Extract all potential answers
            )
            
            if not answer_parsed:
                # Handle case where model's answer cannot be parsed
                reward = 0.0
                diagnostic_info["status"] = "answer_parse_failure"
                diagnostic_info["details"] = f"Failed to parse model answer (completion #{idx+1})"
                if verbose:
                    print(f"Warning: Failed to parse model answer (completion #{idx+1})")
            else:
                diagnostic_info["parsed_answer"] = str(answer_parsed)
                
                # Try verification with different methods
                # First, try exact verification
                exact_match = verify(answer_parsed, gold_parsed, method="exact")
                if exact_match:
                    reward = 1.0
                    diagnostic_info["status"] = "correct_exact_match"
                    diagnostic_info["details"] = "Answer exactly matches the reference solution"
                    diagnostic_info["verification_method"] = "exact"
                else:
                    # Try symbolic verification
                    symbolic_match = verify(answer_parsed, gold_parsed, method="symbolic")
                    if symbolic_match:
                        reward = 1.0
                        diagnostic_info["status"] = "correct_symbolic_match"
                        diagnostic_info["details"] = "Answer is symbolically equivalent to the reference solution"
                        diagnostic_info["verification_method"] = "symbolic"
                    else:
                        # Try numerical verification for numeric problems
                        try:
                            numeric_match = verify(answer_parsed, gold_parsed, method="numeric")
                            if numeric_match:
                                reward = 1.0
                                diagnostic_info["status"] = "correct_numeric_match"
                                diagnostic_info["details"] = "Answer is numerically equivalent to the reference solution"
                                diagnostic_info["verification_method"] = "numeric"
                            else:
                                reward = 0.0
                                diagnostic_info["status"] = "incorrect"
                                diagnostic_info["details"] = "Answer is not equivalent to the reference solution"
                                diagnostic_info["verification_method"] = "all_failed"
                        except Exception as e:
                            # If numerical verification fails, fall back to 0.0
                            reward = 0.0
                            diagnostic_info["status"] = "verification_error"
                            diagnostic_info["details"] = f"Error during verification: {str(e)}"
                            diagnostic_info["verification_method"] = "error"
        
        diagnostic_info["reward"] = reward
        rewards.append(reward)
        diagnostics.append(diagnostic_info)
    
    if return_diagnostics:
        return diagnostics
    return rewards