# deepseekr1maker/rewards/repetition_reward.py
from typing import List, Dict, Any, Callable, Optional, Union
from collections import Counter
import re

def get_repetition_penalty_reward(
    ngram_size: int = 3, 
    max_penalty: float = -0.1,
    min_repetitions: int = 2
) -> Callable:
    """
    Returns a repetition penalty reward function. Penalizes repetitions of n-grams
    in the generated text.
    
    Args:
        ngram_size: Size of n-grams to check
        max_penalty: Maximum (negative) penalty for repetition
        min_repetitions: Minimum number of repetitions before applying penalty
    
    Returns:
        Repetition penalty reward function
    """
    if max_penalty > 0:
        raise ValueError(f"max_penalty {max_penalty} should not be positive")

    def repetition_penalty_reward(completions, return_diagnostics=False, **kwargs) -> Union[List[float], List[Dict[str, Any]]]:
        """
        Repetition penalty reward function that penalizes text with repeated n-grams.
        
        Args:
            completions: List of completions generated by the model
            return_diagnostics: Whether to return detailed diagnostic information
            **kwargs: Additional arguments including:
                - token_type: How to tokenize text ('word', 'char', 'subword', default: 'word')
                - case_sensitive: Whether n-gram matching should be case-sensitive (default: False)
                - threshold: Minimum repetition ratio to apply penalties (default: 0.1)
                - ignore_patterns: List of regex patterns to ignore (e.g., common phrases)
                - normalize_whitespace: Whether to normalize whitespace (default: True)
        
        Returns:
            If return_diagnostics=False:
                List of penalty scores (negative values, closer to 0 is better)
            If return_diagnostics=True:
                List of dictionaries containing penalty scores and diagnostic information
        """
        # Extract configuration from kwargs
        token_type = kwargs.get("token_type", "word")
        case_sensitive = kwargs.get("case_sensitive", False)
        threshold = kwargs.get("threshold", 0.1)
        ignore_patterns = kwargs.get("ignore_patterns", [])
        normalize_whitespace = kwargs.get("normalize_whitespace", True)
        
        # Extract content from completions
        contents = [completion[0]["content"] for completion in completions]
        
        # Prepare results
        rewards = []
        diagnostics = []
        
        for idx, completion in enumerate(contents):
            # Initialize diagnostic info
            diagnostic_info = {
                "penalty": 0.0,
                "ngram_size": ngram_size,
                "total_ngrams": 0,
                "unique_ngrams": 0,
                "repetition_ratio": 0.0,
                "top_repeated_ngrams": [],
                "details": ""
            }
            
            # Handle empty or short completions
            if not completion or len(completion.strip()) == 0:
                diagnostic_info["details"] = "Empty completion, no penalty applied"
                rewards.append(0.0)
                diagnostics.append(diagnostic_info)
                continue
            
            # Normalize text if needed
            text = completion
            if not case_sensitive:
                text = text.lower()
            if normalize_whitespace:
                text = re.sub(r'\s+', ' ', text).strip()
            
            # Apply ignore patterns
            for pattern in ignore_patterns:
                text = re.sub(pattern, " ", text)
            
            # Tokenize based on token_type
            if token_type == "word":
                tokens = text.split()
            elif token_type == "char":
                tokens = list(text)
            elif token_type == "subword":
                # A simple subword tokenization (for illustration)
                tokens = []
                for word in text.split():
                    if len(word) > 4:  # Split longer words
                        tokens.extend([word[i:i+3] for i in range(0, len(word)-2)])
                    else:
                        tokens.append(word)
            else:
                tokens = text.split()  # Default to word tokenization
            
            # Skip if too short
            if len(tokens) < ngram_size:
                diagnostic_info["details"] = f"Completion too short ({len(tokens)} tokens), no penalty applied"
                rewards.append(0.0)
                diagnostics.append(diagnostic_info)
                continue
            
            # Generate n-grams and count occurrences
            ngrams = []
            for i in range(len(tokens) - ngram_size + 1):
                ngram = tuple(tokens[i:i+ngram_size])
                ngrams.append(ngram)
            
            # Count occurrences of each n-gram
            ngram_counts = Counter(ngrams)
            
            # Calculate repetition statistics
            total_ngrams = len(ngrams)
            unique_ngrams = len(ngram_counts)
            
            # Find repeated n-grams (count > 1)
            repeated_ngrams = {ng: count for ng, count in ngram_counts.items() if count >= min_repetitions}
            
            # Calculate repetition ratio
            repetition_ratio = 0.0
            if total_ngrams > 0:
                # This calculates how many n-grams are repetitions
                repetition_count = sum(count - 1 for count in ngram_counts.values() if count > 1)
                repetition_ratio = repetition_count / total_ngrams
            
            # Apply penalty only if above threshold
            penalty = 0.0
            if repetition_ratio > threshold:
                # Scale penalty based on repetition ratio
                # More repetition = more negative penalty
                penalty = max_penalty * min(1.0, (repetition_ratio - threshold) / (1.0 - threshold))
            
            # Get top repeated n-grams for diagnostics
            top_repeated = sorted(
                [(ng, count) for ng, count in ngram_counts.items() if count >= min_repetitions],
                key=lambda x: x[1], 
                reverse=True
            )[:5]  # Top 5 most repeated
            
            # Convert n-grams to readable strings for diagnostics
            top_repeated_readable = []
            for ngram, count in top_repeated:
                if token_type == "word" or token_type == "subword":
                    ngram_text = " ".join(ngram)
                else:  # char
                    ngram_text = "".join(ngram)
                top_repeated_readable.append((ngram_text, count))
            
            # Add details to diagnostics
            diagnostic_info["total_ngrams"] = total_ngrams
            diagnostic_info["unique_ngrams"] = unique_ngrams
            diagnostic_info["repetition_ratio"] = repetition_ratio
            diagnostic_info["top_repeated_ngrams"] = top_repeated_readable
            diagnostic_info["penalty"] = penalty
            
            if repetition_ratio <= threshold:
                diagnostic_info["details"] = f"Repetition ratio {repetition_ratio:.3f} below threshold {threshold}, no penalty applied"
            else:
                diagnostic_info["details"] = f"Repetition ratio {repetition_ratio:.3f} above threshold {threshold}, penalty applied"
            
            rewards.append(penalty)
            diagnostics.append(diagnostic_info)
        
        if return_diagnostics:
            return diagnostics
        return rewards
    
    return repetition_penalty_reward


def ngram_diversity_reward(completions, **kwargs):
    """
    A reward function that measures text diversity based on n-gram statistics.
    This function is the positive counterpart to the repetition penalty.
    
    Args:
        completions: List of completions generated by the model
        **kwargs: Additional arguments including:
            - ngram_size: Size of n-grams to analyze (default: 3)
            - token_type: How to tokenize text ('word', 'char', 'subword', default: 'word')
            - case_sensitive: Whether matching should be case-sensitive (default: False)
            - diversity_weight: How much to weight diversity in the score (default: 1.0)
    
    Returns:
        List of diversity scores (0.0 to 1.0, higher is more diverse)
    """
    # Extract configuration from kwargs
    ngram_size = kwargs.get("ngram_size", 3)
    token_type = kwargs.get("token_type", "word")
    case_sensitive = kwargs.get("case_sensitive", False)
    diversity_weight = kwargs.get("diversity_weight", 1.0)
    
    # Create a repetition penalty function with appropriate settings
    repetition_penalty_func = get_repetition_penalty_reward(
        ngram_size=ngram_size, 
        max_penalty=-1.0  # We'll use full scale for internal calculation
    )
    
    # Get repetition penalties with diagnostics
    penalties = repetition_penalty_func(completions, return_diagnostics=True, 
                                      token_type=token_type, 
                                      case_sensitive=case_sensitive)
    
    # Convert penalties to diversity scores
    diversity_scores = []
    for penalty_info in penalties:
        # A repetition ratio of 0 means perfect diversity (score 1.0)
        # A repetition ratio of 1 means no diversity (score 0.0)
        if "repetition_ratio" in penalty_info:
            # Apply sigmoid-like scaling to make the middle range more sensitive
            repetition_ratio = penalty_info["repetition_ratio"]
            if repetition_ratio <= 0:
                diversity = 1.0
            else:
                # Convert repetition ratio to diversity score (invert and scale)
                diversity = 1.0 - (repetition_ratio ** diversity_weight)
                diversity = max(0.0, min(1.0, diversity))  # Clamp to [0,1]
        else:
            # Default to neutral score if no repetition info
            diversity = 0.5
        
        diversity_scores.append(diversity)
    
    return diversity_scores