# deepseekr1maker/rewards/cosine_reward.py
import math
from typing import List, Dict, Any, Callable, Optional, Union

def get_cosine_scaled_reward(
    min_value_wrong: float = -0.5,
    max_value_wrong: float = -0.1,
    min_value_correct: float = 0.8,
    max_value_correct: float = 1.0,
    max_len: int = 1000,
) -> Callable:
    """
    Returns a cosine-scaled reward function.
    
    This function adjusts the accuracy reward based on completion length.
    Shorter correct solutions receive higher rewards, while longer incorrect
    solutions are less penalized.
    
    Args:
        min_value_wrong: Minimum value for incorrect answers
        max_value_wrong: Maximum value for incorrect answers
        min_value_correct: Minimum value for correct answers
        max_value_correct: Maximum value for correct answers
        max_len: Maximum length for scaling
    
    Returns:
        A cosine-scaled reward function
    """
    def cosine_scaled_reward(completions, solution=None, accuracy_rewards=None, return_diagnostics=False, **kwargs):
        """
        Cosine-scaled reward function that adjusts accuracy rewards based on completion length.
        
        Args:
            completions: List of completions generated by the model
            solution: List of reference solutions (optional)
            accuracy_rewards: List of accuracy rewards (required if solution not provided)
            return_diagnostics: Whether to return detailed diagnostic information
            **kwargs: Additional arguments including:
                - accuracy_function: Function to calculate accuracy rewards if not provided
                - threshold: Threshold to consider an answer correct (default: 0.5)
                - length_penalty_factor: How strongly to apply the length penalty (default: 1.0)
        
        Returns:
            If return_diagnostics=False:
                List of adjusted reward scores
            If return_diagnostics=True:
                List of dictionaries containing reward scores and diagnostic information
        """
        # Extract content from completions
        contents = [completion[0]["content"] for completion in completions]
        
        # Get configuration
        threshold = kwargs.get("threshold", 0.5)
        length_penalty_factor = kwargs.get("length_penalty_factor", 1.0)
        
        # Prepare results
        rewards = []
        diagnostics = []
        
        # If accuracy_rewards are not provided, try to calculate them
        if accuracy_rewards is None:
            accuracy_function = kwargs.get("accuracy_function", None)
            if accuracy_function and solution:
                accuracy_rewards = accuracy_function(completions, solution=solution)
            elif solution is None:
                raise ValueError("Either accuracy_rewards or solution must be provided")
            else:
                raise ValueError("accuracy_function must be provided if accuracy_rewards are not")
        
        # Ensure we have accuracy rewards
        if not accuracy_rewards or len(accuracy_rewards) != len(completions):
            raise ValueError("Invalid accuracy_rewards: length must match completions")
        
        # Process each completion
        for i, (content, acc_reward) in enumerate(zip(contents, accuracy_rewards)):
            diagnostic_info = {
                "accuracy_reward": acc_reward,
                "content_length": len(content),
                "length_ratio": 0,
                "cosine_factor": 0,
                "reward_range": "",
                "details": ""
            }
            
            # Calculate length ratio and cosine factor
            gen_len = len(content)
            progress = min(1.0, gen_len / max_len) * length_penalty_factor
            cosine_factor = math.cos(progress * math.pi)
            
            diagnostic_info["length_ratio"] = progress
            diagnostic_info["cosine_factor"] = cosine_factor
            
            # Determine reward range based on accuracy
            if acc_reward > threshold:
                # Correct answer - reward range from min_value_correct to max_value_correct
                min_value = min_value_correct
                max_value = max_value_correct
                diagnostic_info["reward_range"] = "correct"
                diagnostic_info["details"] = "Answer is correct, scaling reward based on length"
            else:
                # Incorrect answer - reward range from min_value_wrong to max_value_wrong
                # Note the inversion! Shorter incorrect answers are penalized more
                min_value = max_value_wrong
                max_value = min_value_wrong
                diagnostic_info["reward_range"] = "incorrect"
                diagnostic_info["details"] = "Answer is incorrect, scaling penalty based on length"
            
            # Calculate reward using cosine scaling formula
            reward = min_value + 0.5 * (max_value - min_value) * (1.0 + cosine_factor)
            
            # Add explanation of the calculation to diagnostics
            if progress < 0.5:
                length_assessment = "short" if acc_reward > threshold else "short (more penalized)"
            else:
                length_assessment = "long" if acc_reward > threshold else "long (less penalized)"
            
            diagnostic_info["length_assessment"] = length_assessment
            
            # Final reward
            diagnostic_info["reward"] = float(reward)
            rewards.append(float(reward))
            diagnostics.append(diagnostic_info)
        
        if return_diagnostics:
            return diagnostics
        return rewards
    
    return cosine_scaled_reward


def adaptive_cosine_reward(completions, solution=None, accuracy_rewards=None, **kwargs):
    """
    An adaptive version of cosine reward that dynamically adjusts scaling parameters
    based on the problem type and solution characteristics.
    
    Args:
        completions: List of completions generated by the model
        solution: List of reference solutions (optional)
        accuracy_rewards: List of accuracy rewards (required if solution not provided)
        **kwargs: Additional arguments including:
            - problem_type: Type of problem ('math', 'reasoning', etc.)
            - expected_length: Expected response length (overrides automatic detection)
            - prefer_concise: Whether to reward concise answers more strongly (default: True)
    
    Returns:
        List of adjusted reward scores
    """
    # Extract configuration
    problem_type = kwargs.get("problem_type", "general")
    prefer_concise = kwargs.get("prefer_concise", True)
    
    # Determine appropriate scaling parameters based on problem type
    if problem_type == "math":
        # Mathematical problems typically benefit from concise answers
        min_value_wrong = -0.6
        max_value_wrong = -0.2
        min_value_correct = 0.7
        max_value_correct = 1.0
        max_len = kwargs.get("expected_length", 500)  # Math solutions are typically shorter
    elif problem_type == "reasoning":
        # Reasoning problems may require more detailed explanations
        min_value_wrong = -0.5
        max_value_wrong = -0.1
        min_value_correct = 0.8
        max_value_correct = 1.0
        max_len = kwargs.get("expected_length", 2000)  # Reasoning often requires more detail
    elif problem_type == "creative":
        # Creative problems may have varied length requirements
        min_value_wrong = -0.4
        max_value_wrong = -0.2
        min_value_correct = 0.75
        max_value_correct = 1.0
        max_len = kwargs.get("expected_length", 1500)
    else:
        # Default/general parameters
        min_value_wrong = -0.5
        max_value_wrong = -0.1
        min_value_correct = 0.8
        max_value_correct = 1.0
        max_len = kwargs.get("expected_length", 1000)
    
    # If we prefer more concise answers, adjust the scale
    if prefer_concise:
        # Increase reward for shorter correct answers
        min_value_correct = (min_value_correct + max_value_correct) / 2
    else:
        # Reduce penalty for detailed incorrect answers
        max_value_wrong = (min_value_wrong + max_value_wrong) / 2
    
    # Create and call the cosine scaled reward function
    reward_func = get_cosine_scaled_reward(
        min_value_wrong=min_value_wrong,
        max_value_wrong=max_value_wrong,
        min_value_correct=min_value_correct,
        max_value_correct=max_value_correct,
        max_len=max_len,
    )
    
    return reward_func(completions, solution, accuracy_rewards, **kwargs)