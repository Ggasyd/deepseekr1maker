# deepseekr1maker/rewards/language_reward.py
import re
import logging
#from typing import List, Dict, Any, Optional, Union
#from collections import defaultdict
from langdetect import detect_langs

# Language family mappings (useful for partial matching)
LANGUAGE_FAMILIES = {
    # Germanic languages
    "en": "germanic", "de": "germanic", "nl": "germanic", "sv": "germanic", 
    "da": "germanic", "no": "germanic", "is": "germanic",
    
    # Romance languages
    "fr": "romance", "es": "romance", "it": "romance", "pt": "romance", 
    "ro": "romance", "ca": "romance",
    
    # Slavic languages
    "ru": "slavic", "uk": "slavic", "pl": "slavic", "cs": "slavic", 
    "sk": "slavic", "bg": "slavic", "sr": "slavic", "hr": "slavic",
    
    # Other major language families
    "zh": "chinese", "ja": "japanese", "ko": "korean",
    "ar": "arabic", "he": "semitic", "hi": "indic",
    "th": "tai", "vi": "austroasiatic", "id": "austronesian",
    "tr": "turkic", "fa": "iranian"
}

# ISO language codes to language names mapping
LANGUAGE_NAMES = {
    "en": "English", "fr": "French", "es": "Spanish", "de": "German", 
    "it": "Italian", "pt": "Portuguese", "nl": "Dutch", "ru": "Russian",
    "ja": "Japanese", "zh": "Chinese", "ko": "Korean", "ar": "Arabic",
    "hi": "Hindi", "tr": "Turkish", "pl": "Polish", "vi": "Vietnamese",
    "sv": "Swedish", "id": "Indonesian", "uk": "Ukrainian", "cs": "Czech"
}

def detect_language(text, min_length=20):
    """
    Detect the language of a given text using langdetect.
    
    Args:
        text: Text to analyze
        min_length: Minimum text length for reliable detection
        
    Returns:
        Tuple of (language code, confidence score) or (None, 0) if detection fails
    """
    if not text or len(text) < min_length:
        return None, 0
    
    # Remove code blocks, URLs, and other non-natural language content
    cleaned_text = re.sub(r'```.*?```', ' ', text, flags=re.DOTALL)
    cleaned_text = re.sub(r'`.*?`', ' ', cleaned_text)
    cleaned_text = re.sub(r'https?://\S+', ' ', cleaned_text)
    cleaned_text = re.sub(r'\S+@\S+', ' ', cleaned_text)
    
    # Ensure we still have enough text after cleaning
    if len(cleaned_text.strip()) < min_length:
        return None, 0
    
    try:
        langs = detect_langs(cleaned_text)
        if langs:
            return langs[0].lang, langs[0].prob
    except Exception as e:
        logging.warning(f"Language detection failed: {e}")
    
    return None, 0


def language_consistency_reward(completions, return_diagnostics=False, **kwargs):
    """
    Reward function to check if the response is in the same language as the input question.
    Useful for addressing language mixing problems.
    
    Args:
        completions: List of completions generated by the model
        return_diagnostics: Whether to return detailed diagnostic information
        **kwargs: Additional arguments including:
            - input_lang: Language code of the input question
            - user_query: The input question (to detect language if input_lang not provided)
            - allow_family_match: Whether to accept responses in the same language family
            - threshold: Confidence threshold for language detection (default: 0.5)
            - min_length: Minimum text length for reliable detection (default: 20)
            - ignore_code: Whether to ignore code blocks in detection (default: True)
            - multilingual_penalty: Penalty factor for mixed language content (default: 0.5)
    
    Returns:
        If return_diagnostics=False:
            List of reward scores (1.0 for consistent, 0.0 for inconsistent)
        If return_diagnostics=True:
            List of dictionaries containing reward scores and diagnostic information
    """
    # Get configuration from kwargs
    input_lang = kwargs.get("input_lang")
    user_query = kwargs.get("user_query", "")
    allow_family_match = kwargs.get("allow_family_match", False)
    confidence_threshold = kwargs.get("threshold", 0.5)
    min_length = kwargs.get("min_length", 20)
    ignore_code = kwargs.get("ignore_code", True)
    multilingual_penalty = kwargs.get("multilingual_penalty", 0.5)
    
    # Try to detect input language if not provided
    if input_lang is None and user_query:
        input_lang, input_confidence = detect_language(user_query, min_length)
        if input_lang is None or input_confidence < confidence_threshold:
            # Default to English if detection fails or confidence is low
            input_lang = "en"
    elif input_lang is None:
        # Default to English if no user query is available
        input_lang = "en"
    
    # Extract content from completions
    contents = [completion[0]["content"] for completion in completions]
    
    # Prepare results
    rewards = []
    diagnostics = []
    
    # Process each completion
    for content in contents:
        diagnostic_info = {
            "input_language": input_lang,
            "input_language_name": LANGUAGE_NAMES.get(input_lang, "Unknown"),
            "detected_language": None,
            "detected_language_name": "Unknown",
            "confidence": 0.0,
            "family_match": False,
            "reward": 0.0,
            "details": ""
        }
        
        # Skip language detection for very short texts
        if not content or len(content) < min_length:
            reward = 0.5  # Neutral reward for very short content
            diagnostic_info["details"] = f"Content too short ({len(content)} chars) for reliable detection"
            diagnostic_info["reward"] = reward
            rewards.append(reward)
            diagnostics.append(diagnostic_info)
            continue
        
        # Perform language detection
        try:
            detected_lang, confidence = detect_language(content, min_length)
            
            if detected_lang is None:
                # Language detection failed
                reward = 0.5  # Neutral reward
                diagnostic_info["details"] = "Language detection failed"
            elif confidence < confidence_threshold:
                # Low confidence in detection
                reward = 0.5 * confidence / confidence_threshold  # Partial reward based on confidence
                diagnostic_info["detected_language"] = detected_lang
                diagnostic_info["detected_language_name"] = LANGUAGE_NAMES.get(detected_lang, "Unknown")
                diagnostic_info["confidence"] = confidence
                diagnostic_info["details"] = f"Low confidence detection ({confidence:.2f})"
            elif detected_lang == input_lang:
                # Language match - full reward based on confidence
                reward = confidence
                diagnostic_info["detected_language"] = detected_lang
                diagnostic_info["detected_language_name"] = LANGUAGE_NAMES.get(detected_lang, "Unknown")
                diagnostic_info["confidence"] = confidence
                diagnostic_info["details"] = "Language matches input"
            elif allow_family_match and LANGUAGE_FAMILIES.get(detected_lang) == LANGUAGE_FAMILIES.get(input_lang):
                # Language family match - partial reward
                reward = 0.8 * confidence  # 80% reward for family match
                diagnostic_info["detected_language"] = detected_lang
                diagnostic_info["detected_language_name"] = LANGUAGE_NAMES.get(detected_lang, "Unknown")
                diagnostic_info["confidence"] = confidence
                diagnostic_info["family_match"] = True
                diagnostic_info["details"] = f"Language family match: {LANGUAGE_FAMILIES.get(input_lang)}"
            else:
                # Language mismatch - no reward
                reward = 0.0
                diagnostic_info["detected_language"] = detected_lang
                diagnostic_info["detected_language_name"] = LANGUAGE_NAMES.get(detected_lang, "Unknown")
                diagnostic_info["confidence"] = confidence
                diagnostic_info["details"] = f"Language mismatch: expected {input_lang}, got {detected_lang}"
            
            # Check for multilingual content by analyzing different segments
            if len(content) > 200:  # Only check longer content for multiple languages
                segments = re.split(r'\n\n|\.\s+', content)
                segments = [s for s in segments if len(s) >= min_length]
                
                if len(segments) >= 3:  # Need multiple substantial segments
                    # Detect language for each segment
                    segment_langs = {}
                    for segment in segments:
                        lang, conf = detect_language(segment, min_length)
                        if lang and conf >= confidence_threshold:
                            segment_langs[lang] = segment_langs.get(lang, 0) + 1
                    
                    # Check if we have multiple languages
                    if len(segment_langs) > 1:
                        # Calculate proportion of non-main language segments
                        main_lang = max(segment_langs.items(), key=lambda x: x[1])[0]
                        non_main_segments = sum(count for lang, count in segment_langs.items() if lang != main_lang)
                        multilingual_ratio = non_main_segments / len(segments)
                        
                        # Apply multilingual penalty
                        if multilingual_ratio > 0.1:  # More than 10% in different language
                            penalty = multilingual_ratio * multilingual_penalty
                            reward = max(0, reward - penalty)
                            diagnostic_info["details"] += f". Multiple languages detected ({', '.join(segment_langs.keys())})"
        
        except Exception as e:
            # Error in language detection
            reward = 0.5  # Neutral reward
            diagnostic_info["details"] = f"Error in language detection: {str(e)}"
        
        diagnostic_info["reward"] = reward
        rewards.append(reward)
        diagnostics.append(diagnostic_info)
    
    if return_diagnostics:
        return diagnostics
    return rewards